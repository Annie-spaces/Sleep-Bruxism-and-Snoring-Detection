
import joblib
import librosa
import numpy as np
import os
print("å½“å‰å·¥ä½œç›®å½•:", os.getcwd())
print("ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶:", os.listdir(os.getcwd()))

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹

# æŒ‡å®šå®Œæ•´è·¯å¾„
svm_clf = joblib.load(r"C:\Users\lulu\PycharmProjects\Sleep-Bruxism-and-Snoring-Detection\svm_model.pkl")
rf_clf = joblib.load(r"C:\Users\lulu\PycharmProjects\Sleep-Bruxism-and-Snoring-Detection\rf_model.pkl")
meta_clf = joblib.load(r"C:\Users\lulu\PycharmProjects\Sleep-Bruxism-and-Snoring-Detection\stacked_model.pkl")


# åŠ è½½ç‰¹å¾é‡è¦æ€§ï¼Œå¹¶é€‰æ‹©å‰ 10 ä¸ªæœ€é‡è¦çš„ç‰¹å¾
feature_importance = joblib.load("feature_importance.pkl")
top_features = np.argsort(feature_importance)[-10:]

def segment_and_extract_features(audio_path, sr=8000, clip_duration=1.0, step_duration=0.5):
    y, orig_sr = librosa.load(audio_path, sr=None)
    y_resampled = librosa.resample(y, orig_sr=orig_sr, target_sr=sr)

    clip_len = int(sr * clip_duration)
    step_len = int(sr * step_duration)

    features = []
    for start in range(0, len(y_resampled) - clip_len + 1, step_len):
        clip = y_resampled[start: start + clip_len]

        mfccs = librosa.feature.mfcc(y=clip, sr=sr, n_mfcc=13)
        avg_mfcc = np.mean(mfccs, axis=1)

        zcr = librosa.feature.zero_crossing_rate(clip)
        avg_zcr = 10 * np.mean(zcr, axis=1)

        feature_vector = np.hstack([avg_mfcc, avg_zcr])
        features.append(feature_vector)

    return np.array(features)

# æµ‹è¯•éŸ³é¢‘è·¯å¾„
new_audio = r"C:\Users\lulu\Downloads\one_wave.wav"

# åˆ‡ç‰‡å¹¶æå–æ‰€æœ‰ç‰‡æ®µçš„ç‰¹å¾
clips_features = segment_and_extract_features(new_audio)  # shape: (num_clips, 14)
print(f"cut clips get {clips_features.shape[0]} ä¸ªç‰‡æ®µï¼Œevery clips has 14 features")

# å¯¹æ¯ä¸ªç‰‡æ®µé¢„æµ‹
for i, feature in enumerate(clips_features):
    feature_input = feature.reshape(1, -1)

    svm_pred = svm_clf.predict(feature_input)
    rf_pred = rf_clf.predict(feature_input)
    stacked_input = np.column_stack((svm_pred, rf_pred))
    final_pred = meta_clf.predict(stacked_input)

    print(f"part {i+1:02d} second â¤ {'Snoring' if final_pred[0] == 1 else 'No Snoring'}")


# def extract_features_from_audio(file_path):
#     """
#     ä»æœ¬åœ°éŸ³é¢‘æ–‡ä»¶æå– MFCC + è¿‡é›¶ç‡ç‰¹å¾ã€‚
#     """
#     y, sr = librosa.load(file_path, sr=8000)  # è¯»å–éŸ³é¢‘ï¼Œå¹¶é‡é‡‡æ ·è‡³ 8kHz
#
#     # æå– MFCCï¼ˆ13 ç»´ç‰¹å¾ï¼‰
#     mfccs = librosa.feature.mfcc(y=y, sr=8000, n_mfcc=13)
#     avg_mfcc = np.mean(mfccs, axis=1)
#
#     # æå–è¿‡é›¶ç‡ï¼ˆ1 ç»´ç‰¹å¾ï¼‰
#     zero_crossing_rate = librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512)
#     avg_zero_crossing_rate = 10 * np.mean(zero_crossing_rate, axis=1)
#
#     # ç»„åˆç‰¹å¾ï¼ˆ14 ç»´ï¼‰
#     feature = np.hstack([avg_mfcc, avg_zero_crossing_rate])
#     return feature
#
# # æµ‹è¯•æ–°çš„éšæœºéŸ³é¢‘æ–‡ä»¶
# new_audio ="C:/Users/xuany/Desktop/snoring_test2.wav"  # æ›¿æ¢ä¸ºä½ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
# new_features = extract_features_from_audio(new_audio)
#
# # ä»…ä¿ç•™å‰ 10 ä¸ªæœ€é‡è¦çš„ç‰¹å¾
# new_features_reduced = new_features.reshape(1, -1)  # ä½¿ç”¨å®Œæ•´ 14 ç»´ç‰¹å¾
#
#
# # ä½¿ç”¨ SVM å’Œ éšæœºæ£®æ— è¿›è¡Œé¢„æµ‹
# svm_pred = svm_clf.predict(new_features_reduced)
# rf_pred = rf_clf.predict(new_features_reduced)
#
# # è¿›è¡Œ Stackingï¼ˆèåˆ SVM å’Œ éšæœºæ£®æ—çš„é¢„æµ‹ï¼‰
# stacked_input = np.column_stack((svm_pred, rf_pred))
# final_pred = meta_clf.predict(stacked_input)
#
# # è¾“å‡ºæœ€ç»ˆé¢„æµ‹ç»“æœ
# print(f"ğŸ”¹ SVM prediction: {'snoring' if svm_pred[0] == 1 else 'no snoring'}")
# print(f"ğŸ”¹ random forest prediction: {'snoring' if rf_pred[0] == 1 else 'no snoring'}")
# print(f"ğŸ”¹ Stacked model final prediction: {'snoring' if final_pred[0] == 1 else 'no snoring'}")



# import joblib
# import librosa
# import numpy as np
# import os
# from moviepy.editor import VideoFileClip
#
# # ğŸ”¹ æ˜¾ç¤ºå½“å‰å·¥ä½œç›®å½•
# print("ğŸ”¹ å½“å‰å·¥ä½œç›®å½•:", os.getcwd())
# print("ğŸ”¹ ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶:", os.listdir(os.getcwd()))
#
# # âœ… 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
# model_files = ["svm_model.pkl", "rf_model.pkl", "stacked_model.pkl", "feature_importance.pkl"]
# missing_files = [f for f in model_files if not os.path.exists(f)]
#
# if missing_files:
#     raise FileNotFoundError(f"âŒ ç¼ºå°‘ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶: {missing_files}\nè¯·å…ˆè¿è¡Œ `train_model.py` è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ï¼")
#
# # âœ… 2. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
# svm_clf = joblib.load("svm_model.pkl")
# rf_clf = joblib.load("rf_model.pkl")
# meta_clf = joblib.load("stacked_model.pkl")
# feature_importance = joblib.load("feature_importance.pkl")
#
# # ğŸ”¹ è·å–å‰ 10 ä¸ªæœ€é‡è¦çš„ç‰¹å¾ç´¢å¼•
# top_features = np.argsort(feature_importance)[-10:]
# from pydub import AudioSegment  # ç”¨äºæ ¼å¼è½¬æ¢
#
# # âœ… 1. å®šä¹‰ `m4a` è½¬ `wav` çš„å‡½æ•°
# def convert_m4a_to_wav(m4a_path):
#     """
#     å°† .m4a è½¬æ¢ä¸º .wavï¼Œå¹¶è¿”å›æ–°è·¯å¾„ã€‚
#     """
#     wav_path = m4a_path.replace(".m4a", ".wav")  # ä¿®æ”¹æ–‡ä»¶æ‰©å±•å
#     if not os.path.exists(wav_path):  # é¿å…é‡å¤è½¬æ¢
#         audio = AudioSegment.from_file(m4a_path, format="m4a")
#         audio.export(wav_path, format="wav")
#         print(f"âœ… è½¬æ¢å®Œæˆ: {wav_path}")
#     return wav_path  # è¿”å›è½¬æ¢åçš„ .wav æ–‡ä»¶è·¯å¾„
#
# # âœ… 2. é€‰æ‹©è¾“å…¥æ–‡ä»¶ï¼ˆæ”¯æŒ `.wav` å’Œ `.m4a`ï¼‰
# input_path = r"C:\Users\xuany\Desktop\æ‰“é¼¾æµ‹è¯•1.m4a"  # æ›¿æ¢ä¸ºä½ çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
#
# # å¦‚æœæ˜¯ `.m4a`ï¼Œå…ˆè½¬æ¢ä¸º `.wav`
# if input_path.endswith(".m4a"):
#     input_path = convert_m4a_to_wav(input_path)
# # âœ… 3. ä»è§†é¢‘æå–éŸ³é¢‘ï¼ˆå¦‚æœè¾“å…¥æ˜¯ `.mp4`ï¼‰
# def extract_audio_from_video(video_path, output_audio_path):
#     """ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘ï¼Œå¹¶ä¿å­˜ä¸º WAV æ–‡ä»¶ã€‚"""
#     if not os.path.exists(output_audio_path):  # é¿å…é‡å¤è½¬æ¢
#         video = VideoFileClip(video_path)
#         video.audio.write_audiofile(output_audio_path, codec='pcm_s16le', fps=16000)
#         print(f"âœ… éŸ³é¢‘æå–å®Œæˆ: {output_audio_path}")
#
# # âœ… 4. ä»éŸ³é¢‘æ–‡ä»¶æå–ç‰¹å¾
# def extract_features_from_audio(file_path):
#     """ä»æŒ‡å®šçš„éŸ³é¢‘æ–‡ä»¶æå– MFCC + è¿‡é›¶ç‡ç‰¹å¾ã€‚"""
#     y, sr = librosa.load(file_path, sr=8000)  # è¯»å–éŸ³é¢‘ï¼Œå¹¶é‡é‡‡æ ·è‡³ 8kHz
#     mfccs = librosa.feature.mfcc(y=y, sr=8000, n_mfcc=13)
#     avg_mfcc = np.mean(mfccs, axis=1)
#     zero_crossing_rate = librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512)
#     avg_zero_crossing_rate = 10 * np.mean(zero_crossing_rate, axis=1)
#     return np.hstack([avg_mfcc, avg_zero_crossing_rate])  # ç»„åˆ 14 ç»´ç‰¹å¾
#
# # âœ… 5. é€‰æ‹©è¾“å…¥æ–‡ä»¶ï¼ˆæ”¯æŒ `.wav` å’Œ `.mp4`ï¼‰
# input_path = "C:/Users/xuany/Desktop/snoring_test2.mp4"  # å¯ä»¥æ˜¯ MP4 æˆ– WAV
# if input_path.endswith(".mp4"):
#     wav_path = input_path.replace(".mp4", ".wav")
#     extract_audio_from_video(input_path, wav_path)
#     input_path = wav_path  # æ›´æ–°ä¸º WAV è·¯å¾„
#
# # âœ… 6. æå–éŸ³é¢‘ç‰¹å¾
# new_features = extract_features_from_audio(input_path)
# new_features_reduced = new_features.reshape(1, -1)  # ç¡®ä¿æ˜¯ 14 ç»´
#
# # âœ… 7. è¿›è¡Œé¢„æµ‹
# svm_pred = svm_clf.predict(new_features_reduced)
# rf_pred = rf_clf.predict(new_features_reduced)
# stacked_input = np.column_stack((svm_pred, rf_pred))
# final_pred = meta_clf.predict(stacked_input)
#
# # âœ… 8. è¾“å‡ºé¢„æµ‹ç»“æœ
# print(f"ğŸ”¹ SVM prediction: {'snoring' if svm_pred[0] == 1 else 'no snoring'}")
# print(f"ğŸ”¹ Random Forest prediction: {'snoring' if rf_pred[0] == 1 else 'no snoring'}")
# print(f"ğŸ”¹ Stacked Model final prediction: {'snoring' if final_pred[0] == 1 else 'no snoring'}")
